{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0efdd7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/crimedata_csv_AllNeighbourhoods_AllYears.csv')\n",
    "df.columns = map(str.lower, df.columns)\n",
    "num_null_types = df['type'].isnull().sum()\n",
    "print(f\"Number of rows with null 'type': {num_null_types}\")\n",
    "p = pyproj.Proj(proj='utm', zone=10, ellps='WGS84', datum='WGS84', units='m', hemisphere=\"north\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77aba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon, lat = p(df['x'].values, df['y'].values, inverse=True)\n",
    "latitude, longitude = p(lon, lat)\n",
    "df2 = pd.DataFrame(np.c_[lat, lon], columns=['Latitude', 'Longitude'])\n",
    "df = df[~((df['x'] == 0.0) | (df['y'] == 0.0))]\n",
    "print(df.head())\n",
    "df = pd.concat([df, df2], axis=1)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed3ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicate rows, considering only latitude and longitude, but keep one instance of each duplicate\n",
    "duplicate_rows = df.duplicated(subset=['Latitude', 'Longitude'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique duplicate pairs\n",
    "num_duplicate_pairs = duplicate_rows.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d18072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints number of unique duplicate pairs\n",
    "print(\"Number of unique duplicate pairs based on Latitude and Longitude: \", num_duplicate_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f49643",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiveYearsPrior = df['year'].max() - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude data before fiveYearsPrior\n",
    "df = df[df['year'] >= fiveYearsPrior]\n",
    "print(df['year'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_severities = {\n",
    "    'Homicide': 10,\n",
    "    'Vehicle Collision or Pedestrian Struck (with Fatality)': 9,\n",
    "    'Offence Against a Person': 8,\n",
    "    'Break and Enter Commercial': 7,\n",
    "    'Break and Enter Residential/Other': 6,\n",
    "    'Vehicle Collision or Pedestrian Struck (with Injury)': 5,\n",
    "    'Theft of Vehicle': 4,\n",
    "    'Theft from Vehicle': 3,\n",
    "    'Theft of Bicycle': 3,\n",
    "    'Other Theft': 2,\n",
    "    'Mischief': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map crime types to their severities to create the 'weight' column\n",
    "df['weight'] = df['type'].map(crime_severities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb91748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Latitude and Longitude, sum the weights of each group\n",
    "grouped_df = df.groupby(['Latitude', 'Longitude'])['weight'].mean().reset_index(name='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b0b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print to check your new DataFrame\n",
    "print(grouped_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b209cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped_df['weight'].median())\n",
    "print(grouped_df['weight'].max())\n",
    "# Now normalize the 'weight' column to range 1 - 10\n",
    "grouped_df['weight'] = ((grouped_df['weight'] - grouped_df['weight'].min()) /\n",
    "                        (grouped_df['weight'].max() - grouped_df['weight'].min()) *\n",
    "                        (10 - 1)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f99dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print to check your new DataFrame with normalized 'weight'\n",
    "print(grouped_df)\n",
    "print(grouped_df['weight'].quantile(0.5))\n",
    "print(grouped_df['weight'].max())\n",
    "grouped_df.columns = map(str.lower, grouped_df.columns)\n",
    "grouped_df.to_csv('temp_data3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
